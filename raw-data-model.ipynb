{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import mrmr\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = './data/'\n",
    "DATASET_PATH = DATA_PATH + 'smartphone+based+recognition+of+human+activities+and+postural+transitions/'\n",
    "MODELS_PATH = DATA_PATH + 'models/raw-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570929, 8)\n",
      "(244685, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data/train-data.txt\", sep='\\s+', header=None)\n",
    "test_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data/test-data.txt\", sep='\\s+', header=None)\n",
    "\n",
    "train_set.columns=['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]\n",
    "test_set.columns=['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class RawDataModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.Softmax()  # or nn.Softmax(dim=1) for multi-class classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = RawDataModel().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing function\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y = y.squeeze()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.squeeze()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class HAPTDataset(Dataset):\n",
    "    def __init__(self, dataset, features, label):\n",
    "        self.data = torch.tensor(dataset[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataset[label].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = HAPTDataset(train_set, train_set.columns[:-1], 'activity')\n",
    "test_dataset = HAPTDataset(test_set, test_set.columns[:-1], 'activity')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.082390  [    8/570929]\n",
      "loss: 1.736721  [  808/570929]\n",
      "loss: 1.756289  [ 1608/570929]\n",
      "loss: 2.015700  [ 2408/570929]\n",
      "loss: 1.994574  [ 3208/570929]\n",
      "loss: 1.740266  [ 4008/570929]\n",
      "loss: 1.823089  [ 4808/570929]\n",
      "loss: 2.069905  [ 5608/570929]\n",
      "loss: 1.969811  [ 6408/570929]\n",
      "loss: 1.915291  [ 7208/570929]\n",
      "loss: 1.803608  [ 8008/570929]\n",
      "loss: 1.784012  [ 8808/570929]\n",
      "loss: 1.633013  [ 9608/570929]\n",
      "loss: 1.759413  [10408/570929]\n",
      "loss: 1.506513  [11208/570929]\n",
      "loss: 1.979693  [12008/570929]\n",
      "loss: 1.612426  [12808/570929]\n",
      "loss: 1.426604  [13608/570929]\n",
      "loss: 1.987725  [14408/570929]\n",
      "loss: 1.549052  [15208/570929]\n",
      "loss: 1.817516  [16008/570929]\n",
      "loss: 1.684618  [16808/570929]\n",
      "loss: 1.598332  [17608/570929]\n",
      "loss: 1.726735  [18408/570929]\n",
      "loss: 1.697989  [19208/570929]\n",
      "loss: 1.328940  [20008/570929]\n",
      "loss: 1.733185  [20808/570929]\n",
      "loss: 1.413156  [21608/570929]\n",
      "loss: 1.701856  [22408/570929]\n",
      "loss: 2.052372  [23208/570929]\n",
      "loss: 1.800381  [24008/570929]\n",
      "loss: 2.055217  [24808/570929]\n",
      "loss: 1.812707  [25608/570929]\n",
      "loss: 1.531088  [26408/570929]\n",
      "loss: 1.727355  [27208/570929]\n",
      "loss: 2.030099  [28008/570929]\n",
      "loss: 1.336066  [28808/570929]\n",
      "loss: 1.353659  [29608/570929]\n",
      "loss: 1.477093  [30408/570929]\n",
      "loss: 1.606871  [31208/570929]\n",
      "loss: 1.738771  [32008/570929]\n",
      "loss: 1.746869  [32808/570929]\n",
      "loss: 1.494771  [33608/570929]\n",
      "loss: 1.906546  [34408/570929]\n",
      "loss: 1.585576  [35208/570929]\n",
      "loss: 1.769994  [36008/570929]\n",
      "loss: 1.648215  [36808/570929]\n",
      "loss: 1.677572  [37608/570929]\n",
      "loss: 1.979376  [38408/570929]\n",
      "loss: 1.910523  [39208/570929]\n",
      "loss: 1.713746  [40008/570929]\n",
      "loss: 1.356059  [40808/570929]\n",
      "loss: 1.657460  [41608/570929]\n",
      "loss: 1.686803  [42408/570929]\n",
      "loss: 1.720213  [43208/570929]\n",
      "loss: 1.248736  [44008/570929]\n",
      "loss: 1.713669  [44808/570929]\n",
      "loss: 1.842609  [45608/570929]\n",
      "loss: 1.569398  [46408/570929]\n",
      "loss: 1.507172  [47208/570929]\n",
      "loss: 1.491614  [48008/570929]\n",
      "loss: 1.573269  [48808/570929]\n",
      "loss: 1.196263  [49608/570929]\n",
      "loss: 1.435368  [50408/570929]\n",
      "loss: 1.372272  [51208/570929]\n",
      "loss: 1.258862  [52008/570929]\n",
      "loss: 1.465101  [52808/570929]\n",
      "loss: 1.690741  [53608/570929]\n",
      "loss: 1.325527  [54408/570929]\n",
      "loss: 1.481959  [55208/570929]\n",
      "loss: 1.536590  [56008/570929]\n",
      "loss: 1.656601  [56808/570929]\n",
      "loss: 1.576061  [57608/570929]\n",
      "loss: 1.585676  [58408/570929]\n",
      "loss: 1.696669  [59208/570929]\n",
      "loss: 1.670823  [60008/570929]\n",
      "loss: 1.286360  [60808/570929]\n",
      "loss: 1.214330  [61608/570929]\n",
      "loss: 1.650956  [62408/570929]\n",
      "loss: 1.104353  [63208/570929]\n",
      "loss: 1.647414  [64008/570929]\n",
      "loss: 1.749275  [64808/570929]\n",
      "loss: 1.744855  [65608/570929]\n",
      "loss: 1.574397  [66408/570929]\n",
      "loss: 1.605309  [67208/570929]\n",
      "loss: 1.963447  [68008/570929]\n",
      "loss: 1.542025  [68808/570929]\n",
      "loss: 1.461781  [69608/570929]\n",
      "loss: 1.525116  [70408/570929]\n",
      "loss: 1.661210  [71208/570929]\n",
      "loss: 0.858254  [72008/570929]\n",
      "loss: 1.540146  [72808/570929]\n",
      "loss: 1.631393  [73608/570929]\n",
      "loss: 1.350471  [74408/570929]\n",
      "loss: 1.710972  [75208/570929]\n",
      "loss: 1.592003  [76008/570929]\n",
      "loss: 1.534966  [76808/570929]\n",
      "loss: 1.341208  [77608/570929]\n",
      "loss: 1.500726  [78408/570929]\n",
      "loss: 1.287790  [79208/570929]\n",
      "loss: 1.848528  [80008/570929]\n",
      "loss: 1.300146  [80808/570929]\n",
      "loss: 1.428350  [81608/570929]\n",
      "loss: 1.500521  [82408/570929]\n",
      "loss: 1.595487  [83208/570929]\n",
      "loss: 1.578705  [84008/570929]\n",
      "loss: 1.651480  [84808/570929]\n",
      "loss: 1.736716  [85608/570929]\n",
      "loss: 1.622400  [86408/570929]\n",
      "loss: 1.500878  [87208/570929]\n",
      "loss: 1.392008  [88008/570929]\n",
      "loss: 1.435434  [88808/570929]\n",
      "loss: 1.303029  [89608/570929]\n",
      "loss: 1.501677  [90408/570929]\n",
      "loss: 1.694341  [91208/570929]\n",
      "loss: 1.288591  [92008/570929]\n",
      "loss: 1.323047  [92808/570929]\n",
      "loss: 1.138200  [93608/570929]\n",
      "loss: 1.701375  [94408/570929]\n",
      "loss: 1.607969  [95208/570929]\n",
      "loss: 1.442323  [96008/570929]\n",
      "loss: 1.517323  [96808/570929]\n",
      "loss: 1.557140  [97608/570929]\n",
      "loss: 1.412643  [98408/570929]\n",
      "loss: 1.712565  [99208/570929]\n",
      "loss: 1.486816  [100008/570929]\n",
      "loss: 1.114472  [100808/570929]\n",
      "loss: 1.508109  [101608/570929]\n",
      "loss: 1.146318  [102408/570929]\n",
      "loss: 1.585463  [103208/570929]\n",
      "loss: 1.590024  [104008/570929]\n",
      "loss: 1.012882  [104808/570929]\n",
      "loss: 1.528068  [105608/570929]\n",
      "loss: 1.579023  [106408/570929]\n",
      "loss: 1.635730  [107208/570929]\n",
      "loss: 1.820938  [108008/570929]\n",
      "loss: 1.525853  [108808/570929]\n",
      "loss: 1.687852  [109608/570929]\n",
      "loss: 1.691614  [110408/570929]\n",
      "loss: 1.548712  [111208/570929]\n",
      "loss: 1.437917  [112008/570929]\n",
      "loss: 1.572160  [112808/570929]\n",
      "loss: 1.531232  [113608/570929]\n",
      "loss: 1.588994  [114408/570929]\n",
      "loss: 1.702753  [115208/570929]\n",
      "loss: 1.280810  [116008/570929]\n",
      "loss: 1.488552  [116808/570929]\n",
      "loss: 1.196776  [117608/570929]\n",
      "loss: 1.373722  [118408/570929]\n",
      "loss: 1.637714  [119208/570929]\n",
      "loss: 1.239782  [120008/570929]\n",
      "loss: 1.428323  [120808/570929]\n",
      "loss: 1.277514  [121608/570929]\n",
      "loss: 1.791320  [122408/570929]\n",
      "loss: 1.595171  [123208/570929]\n",
      "loss: 1.729711  [124008/570929]\n",
      "loss: 1.416133  [124808/570929]\n",
      "loss: 1.877230  [125608/570929]\n",
      "loss: 1.614207  [126408/570929]\n",
      "loss: 1.883231  [127208/570929]\n",
      "loss: 1.352603  [128008/570929]\n",
      "loss: 1.692464  [128808/570929]\n",
      "loss: 1.454143  [129608/570929]\n",
      "loss: 1.368046  [130408/570929]\n",
      "loss: 1.365644  [131208/570929]\n",
      "loss: 1.590986  [132008/570929]\n",
      "loss: 1.156691  [132808/570929]\n",
      "loss: 1.618835  [133608/570929]\n",
      "loss: 1.390087  [134408/570929]\n",
      "loss: 1.591449  [135208/570929]\n",
      "loss: 1.489974  [136008/570929]\n",
      "loss: 1.484026  [136808/570929]\n",
      "loss: 1.512021  [137608/570929]\n",
      "loss: 1.154799  [138408/570929]\n",
      "loss: 1.513281  [139208/570929]\n",
      "loss: 1.339681  [140008/570929]\n",
      "loss: 1.165781  [140808/570929]\n",
      "loss: 1.571106  [141608/570929]\n",
      "loss: 1.797769  [142408/570929]\n",
      "loss: 1.428031  [143208/570929]\n",
      "loss: 1.038601  [144008/570929]\n",
      "loss: 1.604453  [144808/570929]\n",
      "loss: 2.062949  [145608/570929]\n",
      "loss: 1.804916  [146408/570929]\n",
      "loss: 1.434218  [147208/570929]\n",
      "loss: 1.700046  [148008/570929]\n",
      "loss: 1.579924  [148808/570929]\n",
      "loss: 1.677057  [149608/570929]\n",
      "loss: 1.393093  [150408/570929]\n",
      "loss: 1.755267  [151208/570929]\n",
      "loss: 1.764754  [152008/570929]\n",
      "loss: 1.421611  [152808/570929]\n",
      "loss: 1.576410  [153608/570929]\n",
      "loss: 1.486070  [154408/570929]\n",
      "loss: 1.639095  [155208/570929]\n",
      "loss: 1.291345  [156008/570929]\n",
      "loss: 1.492302  [156808/570929]\n",
      "loss: 1.521599  [157608/570929]\n",
      "loss: 1.497976  [158408/570929]\n",
      "loss: 1.354617  [159208/570929]\n",
      "loss: 1.698232  [160008/570929]\n",
      "loss: 1.715003  [160808/570929]\n",
      "loss: 1.474539  [161608/570929]\n",
      "loss: 1.908764  [162408/570929]\n",
      "loss: 1.265534  [163208/570929]\n",
      "loss: 1.524516  [164008/570929]\n",
      "loss: 1.568351  [164808/570929]\n",
      "loss: 1.772687  [165608/570929]\n",
      "loss: 1.280108  [166408/570929]\n",
      "loss: 1.426782  [167208/570929]\n",
      "loss: 1.456285  [168008/570929]\n",
      "loss: 1.611629  [168808/570929]\n",
      "loss: 1.677409  [169608/570929]\n",
      "loss: 1.621128  [170408/570929]\n",
      "loss: 1.239112  [171208/570929]\n",
      "loss: 1.485859  [172008/570929]\n",
      "loss: 1.553554  [172808/570929]\n",
      "loss: 1.882335  [173608/570929]\n",
      "loss: 1.426994  [174408/570929]\n",
      "loss: 1.679698  [175208/570929]\n",
      "loss: 1.531796  [176008/570929]\n",
      "loss: 1.412444  [176808/570929]\n",
      "loss: 1.120817  [177608/570929]\n",
      "loss: 1.399370  [178408/570929]\n",
      "loss: 1.458200  [179208/570929]\n",
      "loss: 1.460629  [180008/570929]\n",
      "loss: 1.589771  [180808/570929]\n",
      "loss: 1.417030  [181608/570929]\n",
      "loss: 1.238740  [182408/570929]\n",
      "loss: 1.816335  [183208/570929]\n",
      "loss: 1.442672  [184008/570929]\n",
      "loss: 1.361573  [184808/570929]\n",
      "loss: 1.398912  [185608/570929]\n",
      "loss: 1.544367  [186408/570929]\n",
      "loss: 1.460094  [187208/570929]\n",
      "loss: 1.487932  [188008/570929]\n",
      "loss: 1.865998  [188808/570929]\n",
      "loss: 1.595650  [189608/570929]\n",
      "loss: 1.621750  [190408/570929]\n",
      "loss: 1.344892  [191208/570929]\n",
      "loss: 1.400346  [192008/570929]\n",
      "loss: 1.168826  [192808/570929]\n",
      "loss: 1.291054  [193608/570929]\n",
      "loss: 1.202666  [194408/570929]\n",
      "loss: 1.540816  [195208/570929]\n",
      "loss: 1.496803  [196008/570929]\n",
      "loss: 0.970245  [196808/570929]\n",
      "loss: 1.642602  [197608/570929]\n",
      "loss: 1.524142  [198408/570929]\n",
      "loss: 1.493727  [199208/570929]\n",
      "loss: 1.495894  [200008/570929]\n",
      "loss: 1.626736  [200808/570929]\n",
      "loss: 1.214270  [201608/570929]\n",
      "loss: 1.375209  [202408/570929]\n",
      "loss: 1.726135  [203208/570929]\n",
      "loss: 1.723461  [204008/570929]\n",
      "loss: 1.151897  [204808/570929]\n",
      "loss: 1.648783  [205608/570929]\n",
      "loss: 1.544743  [206408/570929]\n",
      "loss: 1.213951  [207208/570929]\n",
      "loss: 1.505472  [208008/570929]\n",
      "loss: 1.645339  [208808/570929]\n",
      "loss: 1.702379  [209608/570929]\n",
      "loss: 1.133789  [210408/570929]\n",
      "loss: 1.164218  [211208/570929]\n",
      "loss: 1.487363  [212008/570929]\n",
      "loss: 1.670347  [212808/570929]\n",
      "loss: 1.860281  [213608/570929]\n",
      "loss: 1.258711  [214408/570929]\n",
      "loss: 1.439710  [215208/570929]\n",
      "loss: 1.201685  [216008/570929]\n",
      "loss: 1.111152  [216808/570929]\n",
      "loss: 1.576892  [217608/570929]\n",
      "loss: 1.133821  [218408/570929]\n",
      "loss: 1.374437  [219208/570929]\n",
      "loss: 1.410326  [220008/570929]\n",
      "loss: 1.405316  [220808/570929]\n",
      "loss: 1.406728  [221608/570929]\n",
      "loss: 1.511103  [222408/570929]\n",
      "loss: 1.337993  [223208/570929]\n",
      "loss: 1.881039  [224008/570929]\n",
      "loss: 1.759103  [224808/570929]\n",
      "loss: 1.711553  [225608/570929]\n",
      "loss: 1.577055  [226408/570929]\n",
      "loss: 1.110241  [227208/570929]\n",
      "loss: 1.625000  [228008/570929]\n",
      "loss: 1.646617  [228808/570929]\n",
      "loss: 1.534353  [229608/570929]\n",
      "loss: 1.755177  [230408/570929]\n",
      "loss: 1.280175  [231208/570929]\n",
      "loss: 1.284220  [232008/570929]\n",
      "loss: 1.778733  [232808/570929]\n",
      "loss: 1.488864  [233608/570929]\n",
      "loss: 1.103552  [234408/570929]\n",
      "loss: 1.613987  [235208/570929]\n",
      "loss: 1.849354  [236008/570929]\n",
      "loss: 1.463407  [236808/570929]\n",
      "loss: 1.400610  [237608/570929]\n",
      "loss: 1.517628  [238408/570929]\n",
      "loss: 1.569455  [239208/570929]\n",
      "loss: 1.516717  [240008/570929]\n",
      "loss: 1.419984  [240808/570929]\n",
      "loss: 1.200688  [241608/570929]\n",
      "loss: 1.354106  [242408/570929]\n",
      "loss: 1.307284  [243208/570929]\n",
      "loss: 1.383234  [244008/570929]\n",
      "loss: 1.247514  [244808/570929]\n",
      "loss: 1.326850  [245608/570929]\n",
      "loss: 1.369165  [246408/570929]\n",
      "loss: 1.279637  [247208/570929]\n",
      "loss: 1.599443  [248008/570929]\n",
      "loss: 1.211815  [248808/570929]\n",
      "loss: 1.413146  [249608/570929]\n",
      "loss: 1.630663  [250408/570929]\n",
      "loss: 1.631153  [251208/570929]\n",
      "loss: 1.830122  [252008/570929]\n",
      "loss: 1.484169  [252808/570929]\n",
      "loss: 1.398978  [253608/570929]\n",
      "loss: 1.771174  [254408/570929]\n",
      "loss: 1.403912  [255208/570929]\n",
      "loss: 1.695390  [256008/570929]\n",
      "loss: 1.583298  [256808/570929]\n",
      "loss: 1.734515  [257608/570929]\n",
      "loss: 1.303717  [258408/570929]\n",
      "loss: 1.614178  [259208/570929]\n",
      "loss: 1.580711  [260008/570929]\n",
      "loss: 1.299237  [260808/570929]\n",
      "loss: 1.552706  [261608/570929]\n",
      "loss: 1.126804  [262408/570929]\n",
      "loss: 1.502645  [263208/570929]\n",
      "loss: 1.085646  [264008/570929]\n",
      "loss: 1.390603  [264808/570929]\n",
      "loss: 1.411110  [265608/570929]\n",
      "loss: 1.308449  [266408/570929]\n",
      "loss: 1.480170  [267208/570929]\n",
      "loss: 0.967706  [268008/570929]\n",
      "loss: 1.640330  [268808/570929]\n",
      "loss: 1.286832  [269608/570929]\n",
      "loss: 1.415120  [270408/570929]\n",
      "loss: 1.715435  [271208/570929]\n",
      "loss: 1.479610  [272008/570929]\n",
      "loss: 1.510338  [272808/570929]\n",
      "loss: 1.183699  [273608/570929]\n",
      "loss: 1.571391  [274408/570929]\n",
      "loss: 1.423927  [275208/570929]\n",
      "loss: 1.468342  [276008/570929]\n",
      "loss: 1.197148  [276808/570929]\n",
      "loss: 1.655542  [277608/570929]\n",
      "loss: 1.401625  [278408/570929]\n",
      "loss: 1.503319  [279208/570929]\n",
      "loss: 1.694046  [280008/570929]\n",
      "loss: 1.696212  [280808/570929]\n",
      "loss: 1.131544  [281608/570929]\n",
      "loss: 1.404773  [282408/570929]\n",
      "loss: 1.797697  [283208/570929]\n",
      "loss: 1.508904  [284008/570929]\n",
      "loss: 1.624783  [284808/570929]\n",
      "loss: 1.040051  [285608/570929]\n",
      "loss: 1.755236  [286408/570929]\n",
      "loss: 1.727752  [287208/570929]\n",
      "loss: 1.679890  [288008/570929]\n",
      "loss: 0.964512  [288808/570929]\n",
      "loss: 1.333043  [289608/570929]\n",
      "loss: 1.657023  [290408/570929]\n",
      "loss: 1.396957  [291208/570929]\n",
      "loss: 1.404722  [292008/570929]\n",
      "loss: 1.219502  [292808/570929]\n",
      "loss: 1.415783  [293608/570929]\n",
      "loss: 1.266406  [294408/570929]\n",
      "loss: 1.137704  [295208/570929]\n",
      "loss: 1.415222  [296008/570929]\n",
      "loss: 1.587391  [296808/570929]\n",
      "loss: 1.540604  [297608/570929]\n",
      "loss: 1.420982  [298408/570929]\n",
      "loss: 1.423152  [299208/570929]\n",
      "loss: 1.037727  [300008/570929]\n",
      "loss: 1.519337  [300808/570929]\n",
      "loss: 1.416021  [301608/570929]\n",
      "loss: 0.921885  [302408/570929]\n",
      "loss: 1.204679  [303208/570929]\n",
      "loss: 1.276387  [304008/570929]\n",
      "loss: 1.355179  [304808/570929]\n",
      "loss: 1.474742  [305608/570929]\n",
      "loss: 1.425030  [306408/570929]\n",
      "loss: 1.541846  [307208/570929]\n",
      "loss: 1.883576  [308008/570929]\n",
      "loss: 1.207035  [308808/570929]\n",
      "loss: 1.089370  [309608/570929]\n",
      "loss: 1.462588  [310408/570929]\n",
      "loss: 1.747198  [311208/570929]\n",
      "loss: 1.609684  [312008/570929]\n",
      "loss: 1.832911  [312808/570929]\n",
      "loss: 1.425273  [313608/570929]\n",
      "loss: 1.585418  [314408/570929]\n",
      "loss: 1.291852  [315208/570929]\n",
      "loss: 1.297377  [316008/570929]\n",
      "loss: 1.615460  [316808/570929]\n",
      "loss: 1.543632  [317608/570929]\n",
      "loss: 1.082201  [318408/570929]\n",
      "loss: 1.658826  [319208/570929]\n",
      "loss: 1.708946  [320008/570929]\n",
      "loss: 1.065612  [320808/570929]\n",
      "loss: 1.286914  [321608/570929]\n",
      "loss: 1.068301  [322408/570929]\n",
      "loss: 0.971399  [323208/570929]\n",
      "loss: 1.405660  [324008/570929]\n",
      "loss: 1.605546  [324808/570929]\n",
      "loss: 1.449577  [325608/570929]\n",
      "loss: 1.456622  [326408/570929]\n",
      "loss: 1.681776  [327208/570929]\n",
      "loss: 1.538910  [328008/570929]\n",
      "loss: 1.477363  [328808/570929]\n",
      "loss: 1.451360  [329608/570929]\n",
      "loss: 1.632819  [330408/570929]\n",
      "loss: 1.368287  [331208/570929]\n",
      "loss: 1.331439  [332008/570929]\n",
      "loss: 1.612661  [332808/570929]\n",
      "loss: 1.328786  [333608/570929]\n",
      "loss: 1.654813  [334408/570929]\n",
      "loss: 1.238923  [335208/570929]\n",
      "loss: 1.248094  [336008/570929]\n",
      "loss: 1.406909  [336808/570929]\n",
      "loss: 1.258754  [337608/570929]\n",
      "loss: 1.730099  [338408/570929]\n",
      "loss: 1.549333  [339208/570929]\n",
      "loss: 1.608328  [340008/570929]\n",
      "loss: 1.210827  [340808/570929]\n",
      "loss: 1.161736  [341608/570929]\n",
      "loss: 1.822122  [342408/570929]\n",
      "loss: 1.577162  [343208/570929]\n",
      "loss: 1.611042  [344008/570929]\n",
      "loss: 1.491078  [344808/570929]\n",
      "loss: 1.205974  [345608/570929]\n",
      "loss: 1.554039  [346408/570929]\n",
      "loss: 1.511290  [347208/570929]\n",
      "loss: 1.160298  [348008/570929]\n",
      "loss: 1.373002  [348808/570929]\n",
      "loss: 1.120393  [349608/570929]\n",
      "loss: 1.269234  [350408/570929]\n",
      "loss: 1.501410  [351208/570929]\n",
      "loss: 1.577003  [352008/570929]\n",
      "loss: 1.598073  [352808/570929]\n",
      "loss: 1.531409  [353608/570929]\n",
      "loss: 1.511671  [354408/570929]\n",
      "loss: 1.382529  [355208/570929]\n",
      "loss: 1.382838  [356008/570929]\n",
      "loss: 1.585715  [356808/570929]\n",
      "loss: 1.524514  [357608/570929]\n",
      "loss: 1.549766  [358408/570929]\n",
      "loss: 1.565276  [359208/570929]\n",
      "loss: 0.955513  [360008/570929]\n",
      "loss: 1.161679  [360808/570929]\n",
      "loss: 1.841870  [361608/570929]\n",
      "loss: 1.268790  [362408/570929]\n",
      "loss: 1.927213  [363208/570929]\n",
      "loss: 1.613478  [364008/570929]\n",
      "loss: 1.524020  [364808/570929]\n",
      "loss: 1.447242  [365608/570929]\n",
      "loss: 1.129740  [366408/570929]\n",
      "loss: 1.171207  [367208/570929]\n",
      "loss: 1.433815  [368008/570929]\n",
      "loss: 1.519165  [368808/570929]\n",
      "loss: 1.643872  [369608/570929]\n",
      "loss: 1.774450  [370408/570929]\n",
      "loss: 1.125427  [371208/570929]\n",
      "loss: 1.645816  [372008/570929]\n",
      "loss: 1.621317  [372808/570929]\n",
      "loss: 1.406636  [373608/570929]\n",
      "loss: 1.349340  [374408/570929]\n",
      "loss: 1.608497  [375208/570929]\n",
      "loss: 1.401358  [376008/570929]\n",
      "loss: 1.634697  [376808/570929]\n",
      "loss: 1.436397  [377608/570929]\n",
      "loss: 1.579752  [378408/570929]\n",
      "loss: 1.395870  [379208/570929]\n",
      "loss: 1.523644  [380008/570929]\n",
      "loss: 1.509532  [380808/570929]\n",
      "loss: 1.274539  [381608/570929]\n",
      "loss: 1.214636  [382408/570929]\n",
      "loss: 1.737385  [383208/570929]\n",
      "loss: 1.617722  [384008/570929]\n",
      "loss: 1.143984  [384808/570929]\n",
      "loss: 1.639035  [385608/570929]\n",
      "loss: 1.517819  [386408/570929]\n",
      "loss: 1.242482  [387208/570929]\n",
      "loss: 1.399369  [388008/570929]\n",
      "loss: 1.504401  [388808/570929]\n",
      "loss: 1.205437  [389608/570929]\n",
      "loss: 1.490814  [390408/570929]\n",
      "loss: 1.578984  [391208/570929]\n",
      "loss: 1.526119  [392008/570929]\n",
      "loss: 1.402406  [392808/570929]\n",
      "loss: 1.417977  [393608/570929]\n",
      "loss: 1.373806  [394408/570929]\n",
      "loss: 1.262350  [395208/570929]\n",
      "loss: 1.567698  [396008/570929]\n",
      "loss: 1.354336  [396808/570929]\n",
      "loss: 1.484486  [397608/570929]\n",
      "loss: 1.340472  [398408/570929]\n",
      "loss: 1.398602  [399208/570929]\n",
      "loss: 1.606723  [400008/570929]\n",
      "loss: 1.403556  [400808/570929]\n",
      "loss: 1.638625  [401608/570929]\n",
      "loss: 1.274523  [402408/570929]\n",
      "loss: 1.579135  [403208/570929]\n",
      "loss: 1.522056  [404008/570929]\n",
      "loss: 1.978379  [404808/570929]\n",
      "loss: 1.295803  [405608/570929]\n",
      "loss: 1.347577  [406408/570929]\n",
      "loss: 1.376770  [407208/570929]\n",
      "loss: 1.248106  [408008/570929]\n",
      "loss: 1.515085  [408808/570929]\n",
      "loss: 1.513555  [409608/570929]\n",
      "loss: 1.473269  [410408/570929]\n",
      "loss: 1.539213  [411208/570929]\n",
      "loss: 1.347540  [412008/570929]\n",
      "loss: 1.577010  [412808/570929]\n",
      "loss: 1.329481  [413608/570929]\n",
      "loss: 1.615761  [414408/570929]\n",
      "loss: 1.247332  [415208/570929]\n",
      "loss: 1.703228  [416008/570929]\n",
      "loss: 1.951757  [416808/570929]\n",
      "loss: 1.292434  [417608/570929]\n",
      "loss: 1.486492  [418408/570929]\n",
      "loss: 1.118050  [419208/570929]\n",
      "loss: 1.458190  [420008/570929]\n",
      "loss: 1.440647  [420808/570929]\n",
      "loss: 1.705677  [421608/570929]\n",
      "loss: 1.486071  [422408/570929]\n",
      "loss: 1.538569  [423208/570929]\n",
      "loss: 1.119743  [424008/570929]\n",
      "loss: 1.632493  [424808/570929]\n",
      "loss: 1.708814  [425608/570929]\n",
      "loss: 1.645386  [426408/570929]\n",
      "loss: 1.318554  [427208/570929]\n",
      "loss: 1.328374  [428008/570929]\n",
      "loss: 1.335674  [428808/570929]\n",
      "loss: 1.081568  [429608/570929]\n",
      "loss: 1.284204  [430408/570929]\n",
      "loss: 1.460038  [431208/570929]\n",
      "loss: 1.207561  [432008/570929]\n",
      "loss: 1.549626  [432808/570929]\n",
      "loss: 1.407350  [433608/570929]\n",
      "loss: 1.628989  [434408/570929]\n",
      "loss: 1.734219  [435208/570929]\n",
      "loss: 1.633924  [436008/570929]\n",
      "loss: 1.522168  [436808/570929]\n",
      "loss: 1.054573  [437608/570929]\n",
      "loss: 1.277195  [438408/570929]\n",
      "loss: 1.468562  [439208/570929]\n",
      "loss: 1.657136  [440008/570929]\n",
      "loss: 1.598458  [440808/570929]\n",
      "loss: 1.178608  [441608/570929]\n",
      "loss: 1.046253  [442408/570929]\n",
      "loss: 1.144369  [443208/570929]\n",
      "loss: 1.513080  [444008/570929]\n",
      "loss: 1.069458  [444808/570929]\n",
      "loss: 1.311843  [445608/570929]\n",
      "loss: 1.564764  [446408/570929]\n",
      "loss: 1.274426  [447208/570929]\n",
      "loss: 1.463416  [448008/570929]\n",
      "loss: 1.646886  [448808/570929]\n",
      "loss: 1.193172  [449608/570929]\n",
      "loss: 1.529773  [450408/570929]\n",
      "loss: 1.543235  [451208/570929]\n",
      "loss: 1.322939  [452008/570929]\n",
      "loss: 1.519547  [452808/570929]\n",
      "loss: 1.617098  [453608/570929]\n",
      "loss: 1.614330  [454408/570929]\n",
      "loss: 1.505767  [455208/570929]\n",
      "loss: 1.003789  [456008/570929]\n",
      "loss: 1.550221  [456808/570929]\n",
      "loss: 1.815827  [457608/570929]\n",
      "loss: 1.351288  [458408/570929]\n",
      "loss: 1.477124  [459208/570929]\n",
      "loss: 1.185956  [460008/570929]\n",
      "loss: 1.742769  [460808/570929]\n",
      "loss: 1.279051  [461608/570929]\n",
      "loss: 1.498324  [462408/570929]\n",
      "loss: 1.213145  [463208/570929]\n",
      "loss: 1.648021  [464008/570929]\n",
      "loss: 1.364860  [464808/570929]\n",
      "loss: 1.483873  [465608/570929]\n",
      "loss: 1.370206  [466408/570929]\n",
      "loss: 1.417825  [467208/570929]\n",
      "loss: 1.446327  [468008/570929]\n",
      "loss: 1.737533  [468808/570929]\n",
      "loss: 1.422255  [469608/570929]\n",
      "loss: 1.593068  [470408/570929]\n",
      "loss: 1.743638  [471208/570929]\n",
      "loss: 1.245672  [472008/570929]\n",
      "loss: 1.457827  [472808/570929]\n",
      "loss: 1.501264  [473608/570929]\n",
      "loss: 1.251922  [474408/570929]\n",
      "loss: 0.926608  [475208/570929]\n",
      "loss: 2.000582  [476008/570929]\n",
      "loss: 1.408122  [476808/570929]\n",
      "loss: 1.523469  [477608/570929]\n",
      "loss: 1.601038  [478408/570929]\n",
      "loss: 1.119671  [479208/570929]\n",
      "loss: 1.657982  [480008/570929]\n",
      "loss: 1.518420  [480808/570929]\n",
      "loss: 1.350954  [481608/570929]\n",
      "loss: 1.638922  [482408/570929]\n",
      "loss: 1.220317  [483208/570929]\n",
      "loss: 1.444426  [484008/570929]\n",
      "loss: 1.356472  [484808/570929]\n",
      "loss: 1.401144  [485608/570929]\n",
      "loss: 1.147189  [486408/570929]\n",
      "loss: 1.302078  [487208/570929]\n",
      "loss: 1.950800  [488008/570929]\n",
      "loss: 0.930309  [488808/570929]\n",
      "loss: 1.270374  [489608/570929]\n",
      "loss: 1.296309  [490408/570929]\n",
      "loss: 1.258826  [491208/570929]\n",
      "loss: 1.635605  [492008/570929]\n",
      "loss: 1.758454  [492808/570929]\n",
      "loss: 1.677665  [493608/570929]\n",
      "loss: 1.116918  [494408/570929]\n",
      "loss: 1.535573  [495208/570929]\n",
      "loss: 1.371434  [496008/570929]\n",
      "loss: 1.365984  [496808/570929]\n",
      "loss: 1.216114  [497608/570929]\n",
      "loss: 1.317806  [498408/570929]\n",
      "loss: 1.420381  [499208/570929]\n",
      "loss: 1.378624  [500008/570929]\n",
      "loss: 1.431580  [500808/570929]\n",
      "loss: 1.665226  [501608/570929]\n",
      "loss: 1.283273  [502408/570929]\n",
      "loss: 1.274117  [503208/570929]\n",
      "loss: 1.604121  [504008/570929]\n",
      "loss: 1.153764  [504808/570929]\n",
      "loss: 1.696461  [505608/570929]\n",
      "loss: 1.412974  [506408/570929]\n",
      "loss: 1.765290  [507208/570929]\n",
      "loss: 1.240632  [508008/570929]\n",
      "loss: 1.635239  [508808/570929]\n",
      "loss: 1.637270  [509608/570929]\n",
      "loss: 1.648371  [510408/570929]\n",
      "loss: 1.586703  [511208/570929]\n",
      "loss: 1.218319  [512008/570929]\n",
      "loss: 1.556886  [512808/570929]\n",
      "loss: 1.149170  [513608/570929]\n",
      "loss: 1.190310  [514408/570929]\n",
      "loss: 1.424786  [515208/570929]\n",
      "loss: 1.360469  [516008/570929]\n",
      "loss: 1.260771  [516808/570929]\n",
      "loss: 1.498825  [517608/570929]\n",
      "loss: 1.198559  [518408/570929]\n",
      "loss: 1.530646  [519208/570929]\n",
      "loss: 1.522645  [520008/570929]\n",
      "loss: 1.444295  [520808/570929]\n",
      "loss: 1.538920  [521608/570929]\n",
      "loss: 1.658640  [522408/570929]\n",
      "loss: 1.673946  [523208/570929]\n",
      "loss: 1.022439  [524008/570929]\n",
      "loss: 1.361368  [524808/570929]\n",
      "loss: 1.537962  [525608/570929]\n",
      "loss: 1.263836  [526408/570929]\n",
      "loss: 1.611860  [527208/570929]\n",
      "loss: 1.438703  [528008/570929]\n",
      "loss: 1.821086  [528808/570929]\n",
      "loss: 1.469417  [529608/570929]\n",
      "loss: 1.074302  [530408/570929]\n",
      "loss: 1.212059  [531208/570929]\n",
      "loss: 1.263908  [532008/570929]\n",
      "loss: 1.916218  [532808/570929]\n",
      "loss: 1.384338  [533608/570929]\n",
      "loss: 1.525671  [534408/570929]\n",
      "loss: 1.331627  [535208/570929]\n",
      "loss: 1.441087  [536008/570929]\n",
      "loss: 1.439902  [536808/570929]\n",
      "loss: 1.481063  [537608/570929]\n",
      "loss: 1.224308  [538408/570929]\n",
      "loss: 1.422647  [539208/570929]\n",
      "loss: 1.190265  [540008/570929]\n",
      "loss: 1.679164  [540808/570929]\n",
      "loss: 1.738854  [541608/570929]\n",
      "loss: 1.422007  [542408/570929]\n",
      "loss: 1.348290  [543208/570929]\n",
      "loss: 1.637654  [544008/570929]\n",
      "loss: 1.421377  [544808/570929]\n",
      "loss: 1.539929  [545608/570929]\n",
      "loss: 1.517668  [546408/570929]\n",
      "loss: 1.686296  [547208/570929]\n",
      "loss: 1.063239  [548008/570929]\n",
      "loss: 1.287991  [548808/570929]\n",
      "loss: 1.401761  [549608/570929]\n",
      "loss: 1.749621  [550408/570929]\n",
      "loss: 1.243955  [551208/570929]\n",
      "loss: 1.257481  [552008/570929]\n",
      "loss: 1.183420  [552808/570929]\n",
      "loss: 1.121691  [553608/570929]\n",
      "loss: 1.346896  [554408/570929]\n",
      "loss: 1.495482  [555208/570929]\n",
      "loss: 1.539800  [556008/570929]\n",
      "loss: 1.284809  [556808/570929]\n",
      "loss: 1.646052  [557608/570929]\n",
      "loss: 1.244340  [558408/570929]\n",
      "loss: 1.426976  [559208/570929]\n",
      "loss: 1.537540  [560008/570929]\n",
      "loss: 1.433408  [560808/570929]\n",
      "loss: 1.858849  [561608/570929]\n",
      "loss: 1.250448  [562408/570929]\n",
      "loss: 1.437255  [563208/570929]\n",
      "loss: 1.359378  [564008/570929]\n",
      "loss: 1.407428  [564808/570929]\n",
      "loss: 1.750430  [565608/570929]\n",
      "loss: 1.337555  [566408/570929]\n",
      "loss: 1.514220  [567208/570929]\n",
      "loss: 1.226185  [568008/570929]\n",
      "loss: 1.645169  [568808/570929]\n",
      "loss: 1.472244  [569608/570929]\n",
      "loss: 1.166415  [570408/570929]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     perf_acc \u001b[38;5;241m=\u001b[39m test(test_dataloader, model, loss_function)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(perf_acc)\n",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "perf_timer = time.perf_counter()\n",
    "perf_acc = \"\"\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    perf_acc = test(test_dataloader, model, loss_function)\n",
    "    \n",
    "print(perf_acc)\n",
    "perf_timer = time.perf_counter() - perf_timer\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
