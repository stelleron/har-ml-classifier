{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, welch, find_peaks\n",
    "import glob\n",
    "import pandas as pd"
    "import glob\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting pandas dataframes\n",
    "def plot_df(df):\n",
    "    # Plotting each column with row number as x-axis\n",
    "    df.plot()\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Row Number')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Pandas DataFrame Plot')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ndarray(arr):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(arr, label='Values')\n",
    "    plt.legend()\n",
    "    plt.title('Autocorrelation')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# 2D Array, 53 features per row (after adjustments to features)\n",
    "all_features = np.empty((0, 53))\n",
    "# 1D array, 1 number per row\n",
    "all_answers = list()\n",
    "# List of every file name in RawData\n",
    "file_names = glob.glob(\"RawData\" + '/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd(data, str):\n",
    "    # Calculate the Power Spectral Density (PSD) using Welch's method\n",
    "    frequencies, psd = welch(data, fs=50, nperseg=len(data))\n",
    "\n",
    "    # Find peaks\n",
    "    peaks, _ = find_peaks(psd)\n",
    "\n",
    "    # Extract first 3 peaks by height\n",
    "    sorted_peaks = np.argsort(psd[peaks])[-3:]\n",
    "    first_3_peaks = peaks[sorted_peaks]\n",
    "    first_3_peak_values = psd[first_3_peaks]\n",
    "    \n",
    "    # # Plotting\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.semilogy(frequencies, psd, label='PSD')\n",
    "    # plt.plot(frequencies[first_3_peaks], first_3_peak_values, 'r^', label='Peaks')\n",
    "    # plt.title(f'Power Spectral Density (PSD) with Peaks of {str}')\n",
    "    # plt.xlabel('Frequency [Hz]')\n",
    "    # plt.ylabel('Power/Frequency [dB/Hz]')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # Print the first 3 peaks and valleys along with their frequencies\n",
    "    # print(frequencies[first_3_peaks])\n",
    "    # print(first_3_peak_values)\n",
    "    # print(\"First 3 Peaks:\")\n",
    "    # for i, (freq, val) in enumerate(zip(frequencies[first_3_peaks], first_3_peak_values), 1):\n",
    "    #     print(f\"Peak {i}: Frequency = {freq:.2f} Hz, Value = {val:.2f} dB/Hz\")\n",
    "\n",
    "    # frequencies[first_3_peaks] is location (x-value of a graph), first_3_peak_values is like a y-value\n",
    "    return (frequencies[first_3_peaks], first_3_peak_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWindow(acc_data, gyro_data, activityNum: int):\n",
    "    cur_features_row = list()\n",
    "\n",
    "    # Calculate mean for each direction in acc and gyro (6 total)\n",
    "    mean_acc_x = acc_data[0].mean()\n",
    "    mean_acc_y = acc_data[1].mean()\n",
    "    mean_acc_z = acc_data[2].mean()\n",
    "    mean_gyro_x = gyro_data[0].mean()\n",
    "    mean_gyro_y = gyro_data[1].mean()\n",
    "    mean_gyro_z = gyro_data[2].mean()\n",
    "    means = [mean_acc_x, mean_acc_y, mean_acc_z, mean_gyro_x, mean_gyro_y, mean_gyro_z]\n",
    "    for mean in means:\n",
    "        cur_features_row.append(mean)\n",
    "\n",
    "    # Calculate RMS for all (6 total numbers)\n",
    "    rms_acc_x = np.sqrt(np.mean(acc_data[0]**2))\n",
    "    rms_acc_y = np.sqrt(np.mean(acc_data[1]**2))\n",
    "    rms_acc_z = np.sqrt(np.mean(acc_data[2]**2))\n",
    "    rms_gyro_x = np.sqrt(np.mean(gyro_data[0]**2))\n",
    "    rms_gyro_y = np.sqrt(np.mean(gyro_data[1]**2))\n",
    "    rms_gyro_z = np.sqrt(np.mean(gyro_data[2]**2))\n",
    "    rmses = [rms_acc_x, rms_acc_y, rms_acc_z, rms_gyro_x, rms_gyro_y, rms_gyro_z]\n",
    "    for rms in rmses:\n",
    "        cur_features_row.append(rms)\n",
    "\n",
    "    '''I don't think I'm doing this right'''\n",
    "    # Calculate autocorrelation for all. Height of 1st peak, height of 2nd peak, and position of 2nd peak (18 total numbers)\n",
    "    acc_autocorr_x, acc_autocorr_y, acc_autocorr_z = full_autocorrelation(acc_data)\n",
    "    gyro_autocorr_x, gyro_autocorr_y, gyro_autocorr_z = full_autocorrelation(gyro_data)\n",
    "\n",
    "    #========================================================================================================\n",
    "\n",
    "    #Take the mean of acc_data\n",
    "    mean_acc = np.mean(acc_data,axis=1)\n",
    "    data =(mean_acc) + np.random.normal(0, 0.1, len(mean_acc))  # add noise\n",
    "    data = (data - np.mean(data)) / np.std(data) #normalize data\n",
    "\n",
    "    # Convert to a pandas Series\n",
    "    composite_signal_series = pd.Series(data)\n",
    "\n",
    "    # Calculate autocorrelation for different lags\n",
    "    lags = range(-20, 21)  # Example range of lags from -5 to 5\n",
    "    autocorr_values = [composite_signal_series.autocorr(lag=k) for k in lags]\n",
    "    # Smooth the autocorrelation values using a moving average\n",
    "    window_size = 5  # Size of the smoothing window\n",
    "    smoothed_autocorr_values = np.convolve(autocorr_values, np.ones(window_size)/window_size, mode='valid')\n",
    "    smoothed_lags = lags[(window_size-1)//2: -(window_size//2)]\n",
    "\n",
    "\n",
    "    # Plotting the autocorrelation on the existing plot\n",
    "    plt.plot(smoothed_lags, smoothed_autocorr_values, marker='o', label=activityNum)\n",
    "#=================================================================================================================\n",
    "#========================================================================================================\n",
    "    # # No more autocorrelation\n",
    "    # composite_signal_series = pd.Series(acc_data[0])\n",
    "    # var = composite_signal_series.var()\n",
    "    # print(var)\n",
    "\n",
    "\n",
    "    # # Calculate autocorrelation for different lags\n",
    "    # lags = range(-100, 100)\n",
    "    # autocorr_values = [composite_signal_series.autocorr(lag=k) for k in lags]\n",
    "\n",
    "\n",
    "    # # Plotting the autocorrelation on the existing plot\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(lags, autocorr_values, marker='o', label=activityNum)\n",
    "    # plt.axhline(0, color='black', linewidth=0.5)\n",
    "    # plt.title('Autocorrelation Comparison')\n",
    "    # plt.xlabel('Time lag (s)')\n",
    "    # plt.ylabel('Correlation')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "#=================================================================================================================\n",
    "\n",
    "    # Spectral peaks (36)\n",
    "    multi_tuples = list()\n",
    "    multi_tuples.append(psd(acc_data[0], \"Acc_X\"))\n",
    "    multi_tuples.append(psd(acc_data[1], \"Acc_Y\"))\n",
    "    multi_tuples.append(psd(acc_data[2], \"Acc_Z\"))\n",
    "    multi_tuples.append(psd(gyro_data[0], \"Gyro_X\"))\n",
    "    multi_tuples.append(psd(gyro_data[1], \"Gyro_Y\"))\n",
    "    multi_tuples.append(psd(gyro_data[2], \"Gyro_Z\"))\n",
    "\n",
    "    # Adds all 3 positions of the peak, then all 3 values of the peak\n",
    "    for tuple in multi_tuples:\n",
    "        for arr in tuple:\n",
    "            for data_pt in arr:\n",
    "                cur_features_row.append(data_pt)\n",
    "\n",
    "    # Resultant/magnitude acceleration (1) \n",
    "    acc_mag = np.sqrt(mean_acc_x**2 + mean_acc_y**2 + mean_acc_z**2)\n",
    "    cur_features_row.append(acc_mag)\n",
    "\n",
    "    # Resultant gyro (1)\n",
    "    gyro_mag = np.sqrt(mean_gyro_x**2 + mean_gyro_y**2 + mean_gyro_z**2)\n",
    "    cur_features_row.append(gyro_mag)\n",
    "\n",
    "    # Angle btwn resultant acc and each acc input (3)\n",
    "    angle_accX = np.degrees(np.arccos(mean_acc_x / acc_mag))\n",
    "    angle_accY = np.degrees(np.arccos(mean_acc_y / acc_mag))\n",
    "    angle_accZ = np.degrees(np.arccos(mean_acc_z / acc_mag))\n",
    "    cur_features_row.append(angle_accX)\n",
    "    cur_features_row.append(angle_accY)\n",
    "    cur_features_row.append(angle_accZ)\n",
    "\n",
    "\n",
    "    cur_features_row_as_npArr = np.array(cur_features_row)\n",
    "\n",
    "    global all_features\n",
    "    all_features = np.vstack((all_features, cur_features_row_as_npArr))\n",
    "    all_answers.append(activityNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''These are for noise reduction'''\n",
    "# Function to apply median filter\n",
    "def apply_median_filter(data, window_size):\n",
    "    return data.rolling(window=window_size, center=True, min_periods=1).median()\n",
    "\n",
    "# Function to design a low-pass Butterworth filter\n",
    "def butter_lowpass(cutoff, fs, order):\n",
    "    nyq = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "# Function to apply the Butterworth filter\n",
    "def apply_low_butter(data, cutoff, fs, order):\n",
    "    b, a = butter_lowpass(cutoff, fs, order)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    return y\n",
    "\n",
    "'''This is for getting body acc component from total acc'''\n",
    "# Define a function to create a high-pass Butterworth filter\n",
    "def butter_highpass(cutoff, fs, order):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency is half the sampling rate\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "# Define a function to apply the high-pass filter to data\n",
    "def apply_high_butter(data, cutoff, fs, order):\n",
    "    b, a = butter_highpass(cutoff, fs, order)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For testing'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAllFeatures(label: str):\n",
    "    splitted = label.split()\n",
    "    experimentNum, activityNum, start, end = int(splitted[0]), int(splitted[2]), int(splitted[3]), int(splitted[4])\n",
    "\n",
    "    acc_file_name = file_names[experimentNum-1]\n",
    "    gyro_file_name = file_names[experimentNum-1+61]\n",
    "\n",
    "    # Reading acc and gyro in data for this label range\n",
    "    acc_data = pd.read_csv(acc_file_name, sep=' ', header=None, skiprows=start, nrows=end-start)\n",
    "    gyro_data = pd.read_csv(gyro_file_name, sep=' ', header=None, skiprows=start, nrows=end-start)\n",
    "\n",
    "    ''' I'm not too sure how these filters work but I hope they do'''\n",
    "    # Apply median filter to each axis of accelerometer and gyroscope data (for noise reduction)\n",
    "    WINDOW_SIZE = 3\n",
    "    acc_data_filtered = acc_data.apply(apply_median_filter, window_size=WINDOW_SIZE)\n",
    "    gyro_data_filtered = gyro_data.apply(apply_median_filter, window_size=WINDOW_SIZE)\n",
    "    \n",
    "    # Apply low-pass Butterworth filter to each axis of accelerometer and gyroscope data (for noise reduction)\n",
    "    FS = 50                     # Sampling frequency in Hz (you mentioned 50Hz)\n",
    "    CUTOFF_LOW_PASS = 20        # Desired cutoff frequency of the filter, Hz\n",
    "    ORDER = 3                   # Order of the Butterworth filter\n",
    "    acc_data_filtered = acc_data_filtered.apply(lambda col: apply_low_butter(col, CUTOFF_LOW_PASS, FS, ORDER))\n",
    "    gyro_data_filtered = gyro_data_filtered.apply(lambda col: apply_low_butter(col, CUTOFF_LOW_PASS, FS, ORDER))    \n",
    "\n",
    "    # Apply high-pass Butterworth filter to acc to get body component\n",
    "    CUTOFF_HIGH_PASS = 0.5\n",
    "    acc_data_filtered = acc_data_filtered.apply(lambda col: apply_high_butter(col, CUTOFF_HIGH_PASS, FS, ORDER))\n",
    "    gyro_data_filtered = gyro_data_filtered.apply(lambda col: apply_high_butter(col, CUTOFF_HIGH_PASS, FS, ORDER))\n",
    "\n",
    "    # 2.5sec windows * 50 samples per sec = 125 samples per window\n",
    "    # 50% overlap means we go up by 62 for every new window\n",
    "    # end-62 is used to skip the last half window that gets added\n",
    "    length = end-start\n",
    "    i = 0\n",
    "    # while i < length-62:\n",
    "    #     trueEnd = min(i+125, length)\n",
    "    #     extractWindow(acc_data_filtered[i: trueEnd], gyro_data_filtered[i: trueEnd], activityNum)\n",
    "    #     i += 62\n",
    "    trueEnd = min(i+125, length)\n",
    "    extractWindow(acc_data_filtered[i: trueEnd], gyro_data_filtered[i: trueEnd], activityNum)\n",
    "\n",
    "    \n",
    "\n",
    "'''For testing'''\n",
    "\n",
    "# extractAllFeatures('1 1 4 1393 2194')\n",
    "# extractAllFeatures('1 1 1 7496 8078')\n",
    "# extractAllFeatures('1 1 2 14069 14699')\n",
    "# extractAllFeatures('1 1 3 14869 15492')\n",
    "\n",
    "# extractAllFeatures('1 1 1 7496 8078')\n",
    "# extractAllFeatures('1 1 2 14069 14699')\n",
    "# extractAllFeatures('1 1 3 14869 15492')\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Finalize the plot\n",
    "extractAllFeatures('1 1 5 250 1232')\n",
    "extractAllFeatures('1 1 7 1233 1392')\n",
    "extractAllFeatures('1 1 1 7496 8078')\n",
    "extractAllFeatures('1 1 3 14869 15492')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.title('Autocorrelation Comparison')\n",
    "plt.xlabel('Time lag (s)')\n",
    "plt.ylabel('Correlation')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Grab the labels.txt file telling you what each data section means\n",
    "    labels_file = open('labels.txt') \n",
    "    labels_list = labels_file.readlines()\n",
    "\n",
    "    # For each section: \n",
    "        # i. Load that section's respective data\n",
    "        # ii. For each window in that section:\n",
    "            # a. Apply noise filters on the window\n",
    "            # b. Compute features from the window\n",
    "            # c. Store those feature values in an array\n",
    "    for label in labels_list:\n",
    "        extractAllFeatures(label)\n",
    "    \n",
    "    '''I checked: all_features and all_answers are the same length (same number of rows)'''\n",
    "\n",
    "    # Take all_features and answers, split into test and train, and finally write to .txt file\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_features, all_answers, test_size=0.2, random_state=42)\n",
    "    \n",
    "    np.savetxt(\"X_train.txt\", X_train, delimiter=' ', fmt='%.8f')\n",
    "    np.savetxt(\"X_test.txt\", X_test, delimiter=' ', fmt='%.8f')\n",
    "    np.savetxt(\"y_train.txt\", y_train, delimiter=' ', fmt='%d')\n",
    "    np.savetxt(\"y_test.txt\", y_test, delimiter=' ', fmt='%d')\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
